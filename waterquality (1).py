# -*- coding: utf-8 -*-
"""waterquality.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dAZO7XGrhr6fZf-n8N7TduZytTaR7yaP

#Water Quality 
Access to safe drinking-water is essential to health, a basic human right and a component of effective policy for health protection. This is important as a health and development issue at a national, regional and local level. In some regions, it has been shown that investments in water supply and sanitation can yield a net economic benefit, since the reductions in adverse health effects and health care costs outweigh the costs of undertaking the interventions.
The file include:
1. pH value:PH is an important parameter in evaluating the acid–base balance of water. WHO has recommended maximum permissible limit of pH from 6.5 to 8.5
2. Hardness:Hardness was originally defined as the capacity of water to precipitate soap caused by Calcium and Magnesium.
3. Solids (Total dissolved solids - TDS): The water with high TDS value indicates that water is highly mineralized. Desirable limit for TDS is 500 mg/l and maximum limit is 1000 mg/l.
4. Chloramines: Chloramines are most commonly formed when ammonia is added to chlorine to treat drinking water. Chlorine levels up to 4 milligrams per liter is safe.
5. Sulfate:Sulfates are naturally occurring substances that are found in minerals, soil, and rocks. It ranges from 3 to 30 mg/L in most freshwater supplies
6. Conductivity:the amount of dissolved solids in water determines the electrical conductivity.EC value should not exceeded 400 μS/cm.
7. Organic_carbon:Total Organic Carbon (TOC) in source waters comes from decaying natural organic matter (NOM) as well as synthetic sources. According to US EPA < 2 mg/L as TOC in treated / drinking water, and < 4 mg/Lit in source water which is use for treatment.
8. Trihalomethanes:THMs are chemicals which may be found in water treated with chlorine.THM levels up to 80 ppm is considered safe .
9. Turbidity:The turbidity of water depends on the quantity of solid matter present in the suspended state. WHO recommended value of 5.00 NTU.
10. Potability:Indicates if water is safe for human consumption where 1 means Potable and 0 means Not potable.

The dataset is from [Kaggle](https://www.kaggle.com/adityakadiwal/water-potability) which has the information regarding different water samples.
"""

#importing libraries
import numpy as np # linear algebra
import pandas as pd # data processing
import missingno as msno
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_theme(style="whitegrid")
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingClassifier,AdaBoostClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV,RandomizedSearchCV,RepeatedStratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score,plot_roc_curve

#importing dataset
df = pd.read_csv('water_potability.csv')
df.shape

df.head()

df.info()

df.describe()

df.hist(figsize=(15,15))
plt.show()

"""##Data Cleaning

"""

df.isnull().sum()

#visualizing the null values for each attribute
plt.figure(figsize=(10,5))
sns.heatmap(df.isnull(),yticklabels=False,cbar=False)

"""There are many ways to fill the missing values,like filling it with zero,filling with mean,median or interpolated value. """

#Filling missing value with mean
ndf = df
ndf['ph']=df['ph'].fillna(df['ph'].mean())
ndf['Sulfate']=df['Sulfate'].fillna(df['Sulfate'].mean())
ndf['Trihalomethanes']=df['Trihalomethanes'].fillna(df['Trihalomethanes'].mean())

ndf.isnull().sum()

#Plotting the graph of potability
sns.countplot(x="Potability", data=ndf,palette="Set2")

#Plotting graphs of different columns
plt.figure(figsize=(16,12))
cdf = ndf.drop('Potability',axis=1)
for i, column in enumerate(cdf.columns, 1):
    plt.subplot(3,3,i)
    sns.histplot(df[column])

#3Graph of change in potability with ph
sns.displot(ndf, x="ph", hue="Potability", kind="kde", multiple="stack")

#Potability w.r.t. hardness
sns.displot(ndf, x="Hardness", hue="Potability", kind="kde", multiple="stack")

#potabilty w.r.t solids
sns.displot(ndf, x="Solids", hue="Potability", kind="kde", multiple="stack")

#potability w.r.t chloramines
sns.displot(ndf, x="Chloramines", hue="Potability", kind="kde", multiple="stack")

#potability w.r.t Sulphate
sns.displot(ndf, x="Sulfate", hue="Potability", kind="kde", multiple="stack")

#potability w.r.t Conductivity
sns.displot(ndf, x="Conductivity", hue="Potability", kind="kde", multiple="stack")

plt.figure(figsize=(10,8), dpi= 80)
sns.pairplot(ndf, hue="Potability",diag_kind="hist")
plt.show()

"""Data look well distributed and Normalized."""

#plotting correlation matrix
plt.figure(figsize=(12,10))
sns.heatmap(df.corr(), annot=True)

"""There data is not much correlated."""

sns.jointplot(data=ndf, x="Hardness", y="ph",hue='Potability')

sns.displot(ndf, x="ph", y="Hardness", hue="Potability", kind="kde")

X = ndf.drop('Potability',axis=1)
y=ndf['Potability'].copy()

"""###Scaling the data for better results"""

from sklearn.preprocessing import StandardScaler

ss=StandardScaler()
X= ss.fit_transform(X)
X= pd.DataFrame(X, columns= [col for col in df.columns if col!='Potability' ])

X.head()

"""###Training the data

"""

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size = 0.2,random_state=42,shuffle = True)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

models ={'RandomForestClassifier':RandomForestClassifier(),'GradientBoostingClassifier':GradientBoostingClassifier(),
        'AdaBoostClassifier':AdaBoostClassifier(),'LGBMClassifier':LGBMClassifier(),
         
        }

training_scores= []
testing_scores=[]

for key, value in models.items():
    value.fit(X_train_scaled, y_train)
    train_score= value.score(X_train_scaled,  y_train)
    test_score= value.score(X_test_scaled, y_test)
    training_scores.append(train_score)
    testing_scores.append(test_score)
    
    print(f"{key}\n")
    print("Training Accuracy: {0:.3f}".format(train_score*100))
    print("Training Accuracy: {0:.3f} \n".format(test_score*100))

model = RandomForestClassifier()
n_estimators = [10, 100, 500]
max_features = ['sqrt', 'log2']

# Grid search
grid = dict(n_estimators=n_estimators,max_features=max_features)
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)
grid_result = grid_search.fit(X_train_scaled, y_train)

#Results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

rclf = RandomForestClassifier(max_features='log2',n_estimators= 500)
rclf.fit(X_train_scaled,y_train)
y_pred = rclf.predict(X_test_scaled)
print("Training Accuracy: {0:.3f}".format(rclf.score(X_train_scaled, y_train)*100))
print("Testing Accuracy: {0:.3f}".format(accuracy_score(y_test,y_pred)*100))

model = LGBMClassifier(boosting_type='dart')
max_bin=[255,300,350,450]
lr=[0.01,0.001,0.0001,0.11]
num_leaves=[31,100,250]

grid = dict(max_bin=max_bin,learning_rate =lr,num_leaves=num_leaves)
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3)
grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)
grid_result = grid_search.fit(X_train_scaled, y_train)

#Results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

lgb = LGBMClassifier(learning_rate= 0.11, max_bin= 255, num_leaves= 31,boosting_type='dart')
lgb.fit(X_train_scaled,y_train)
y_pred = lgb.predict(X_test_scaled)
print("Training Accuracy: {0:.3f}".format(lgb.score(X_train_scaled, y_train)*100))
print("Testing Accuracy: {0:.3f}".format(accuracy_score(y_test,y_pred)*100))

models = [lgb, rclf]
ax = plt.gca()
for i in models:
    plot_roc_curve(i, X_test_scaled, y_test, ax=ax)